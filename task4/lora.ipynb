{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8537bd04d550426da2a24eb652278814": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c3a0244006c43d4826feef84774616d",
              "IPY_MODEL_ddccfe049ed846a793536909b291b02d",
              "IPY_MODEL_ffdb291e7b71439ea5ce9681f23fc68b"
            ],
            "layout": "IPY_MODEL_2a0f54134f94400a8d655b9105c9e937"
          }
        },
        "0c3a0244006c43d4826feef84774616d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_77cc285cf24f4bfc881dbaa2ae61ccfe",
            "placeholder": "​",
            "style": "IPY_MODEL_e0455fdf3c7b464e9e895f12007d567a",
            "value": "Loading pipeline components...: 100%"
          }
        },
        "ddccfe049ed846a793536909b291b02d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2812ded5de7a434a934acaca6849e772",
            "max": 7,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93f95b4f79564da9b9635cc9639b61c0",
            "value": 7
          }
        },
        "ffdb291e7b71439ea5ce9681f23fc68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42b3fc1b492745dab67832292e6f7942",
            "placeholder": "​",
            "style": "IPY_MODEL_048fcf2adb77493686f936b608e6ac0f",
            "value": " 7/7 [00:29&lt;00:00,  3.86s/it]"
          }
        },
        "2a0f54134f94400a8d655b9105c9e937": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77cc285cf24f4bfc881dbaa2ae61ccfe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e0455fdf3c7b464e9e895f12007d567a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2812ded5de7a434a934acaca6849e772": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93f95b4f79564da9b9635cc9639b61c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42b3fc1b492745dab67832292e6f7942": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "048fcf2adb77493686f936b608e6ac0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97c7bd5cb61e40c2b3e0d986e68154d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c305d5aa9bcf4bd396cadb5e828fa063",
              "IPY_MODEL_39529781bc084509b54f6910c4ed3a2d",
              "IPY_MODEL_b33deda178e446ffa239a3b6ed3b38af"
            ],
            "layout": "IPY_MODEL_813d85bb2bbb414d806fb01a791b1bd3"
          }
        },
        "c305d5aa9bcf4bd396cadb5e828fa063": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549afad2ec3b475baff0c1c26e42b991",
            "placeholder": "​",
            "style": "IPY_MODEL_b6bb244e8e9e4458a45a40bb3ba5a22d",
            "value": "100%"
          }
        },
        "39529781bc084509b54f6910c4ed3a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d4177cfeaa148feb0a11a03b8b871b8",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3c796bfb7901469fa34de2d826828f34",
            "value": 30
          }
        },
        "b33deda178e446ffa239a3b6ed3b38af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a24483bc2be24dd3b94704b8afaab39e",
            "placeholder": "​",
            "style": "IPY_MODEL_b0dbb5dab30d45e98964baefda505719",
            "value": " 30/30 [00:04&lt;00:00,  6.57it/s]"
          }
        },
        "813d85bb2bbb414d806fb01a791b1bd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "549afad2ec3b475baff0c1c26e42b991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bb244e8e9e4458a45a40bb3ba5a22d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d4177cfeaa148feb0a11a03b8b871b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c796bfb7901469fa34de2d826828f34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a24483bc2be24dd3b94704b8afaab39e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0dbb5dab30d45e98964baefda505719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7xBpWj04BCjr"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/huggingface/diffusers"
      ],
      "metadata": {
        "id": "EGkG3yugos14"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install /content/diffusers/."
      ],
      "metadata": {
        "id": "RIofJQsAoywW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r /content/diffusers/examples/text_to_image/requirements.txt"
      ],
      "metadata": {
        "id": "NL0B6bTVqeYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate config"
      ],
      "metadata": {
        "id": "BaoBZ5leqznj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! huggingface-cli login"
      ],
      "metadata": {
        "id": "IbpLDTWlrGs2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install wandb"
      ],
      "metadata": {
        "id": "HDUFzLq9vwJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch --mixed_precision=\"fp16\" /content/diffusers/examples/text_to_image/train_text_to_image_lora.py \\\n",
        "  --pretrained_model_name_or_path=\"CompVis/stable-diffusion-v1-4\" \\\n",
        "  --dataset_name=\"lambdalabs/pokemon-blip-captions\" \\\n",
        "  --caption_column=\"text\" \\\n",
        "  --resolution=512 \\\n",
        "  --random_flip \\\n",
        "  --train_batch_size=1 \\\n",
        "  --num_train_epochs=50 \\\n",
        "  --checkpointing_steps=5000 \\\n",
        "  --resume_from_checkpoint=\"/content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-25000\"\\\n",
        "  --learning_rate=1e-04 \\\n",
        "  --lr_scheduler=\"constant\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --seed=43 \\\n",
        "  --output_dir=\"/content/drive/MyDrive/sd-pokemon-model-lora\" \\\n",
        "  --validation_prompt=\"cute dragon creature\" \\\n",
        "  --report_to=\"wandb\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A9AT8ag9rll6",
        "outputId": "710dc720-ef2f-4643-c2a4-afbe6ab4e8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-18 02:49:44.927146: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-07-18 02:49:50.781454: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "07/18/2023 02:49:53 - INFO - __main__ - Distributed environment: NO\n",
            "Num processes: 1\n",
            "Process index: 0\n",
            "Local process index: 0\n",
            "Device: cuda\n",
            "\n",
            "Mixed precision type: fp16\n",
            "\n",
            "Downloading (…)cheduler_config.json: 100% 313/313 [00:00<00:00, 2.03MB/s]\n",
            "{'variance_type', 'sample_max_value', 'dynamic_thresholding_ratio', 'timestep_spacing', 'prediction_type', 'thresholding', 'clip_sample_range'} was not found in config. Values will be initialized to default values.\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 6.62MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 76.3MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 2.55MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 806/806 [00:00<00:00, 5.14MB/s]\n",
            "Downloading (…)_encoder/config.json: 100% 592/592 [00:00<00:00, 3.44MB/s]\n",
            "Downloading model.safetensors: 100% 492M/492M [00:01<00:00, 297MB/s]\n",
            "Downloading (…)main/vae/config.json: 100% 551/551 [00:00<00:00, 2.83MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 335M/335M [00:01<00:00, 300MB/s]\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Downloading (…)ain/unet/config.json: 100% 743/743 [00:00<00:00, 3.69MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 3.44G/3.44G [00:19<00:00, 180MB/s]\n",
            "{'class_embeddings_concat', 'only_cross_attention', 'projection_class_embeddings_input_dim', 'upcast_attention', 'mid_block_type', 'num_attention_heads', 'encoder_hid_dim_type', 'addition_embed_type_num_heads', 'num_class_embeds', 'time_embedding_act_fn', 'dual_cross_attention', 'class_embed_type', 'time_embedding_type', 'mid_block_only_cross_attention', 'conv_in_kernel', 'encoder_hid_dim', 'use_linear_projection', 'cross_attention_norm', 'resnet_time_scale_shift', 'resnet_out_scale_factor', 'addition_embed_type', 'time_embedding_dim', 'conv_out_kernel', 'transformer_layers_per_block', 'resnet_skip_time_act', 'addition_time_embed_dim', 'timestep_post_act', 'time_cond_proj_dim'} was not found in config. Values will be initialized to default values.\n",
            "Downloading metadata: 100% 731/731 [00:00<00:00, 6.11MB/s]\n",
            "Downloading readme: 100% 1.80k/1.80k [00:00<00:00, 12.5MB/s]\n",
            "Downloading and preparing dataset imagefolder/pokemon (download: 95.05 MiB, generated: 113.89 MiB, post-processed: Unknown size, total: 208.94 MiB) to /root/.cache/huggingface/datasets/lambdalabs___parquet/lambdalabs--pokemon-blip-captions-10e3527a764857bd/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
            "Downloading data files:   0% 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0% 0.00/99.7M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:  11% 11.0M/99.7M [00:00<00:00, 110MB/s]\u001b[A\n",
            "Downloading data:  23% 23.2M/99.7M [00:00<00:00, 117MB/s]\u001b[A\n",
            "Downloading data:  35% 35.4M/99.7M [00:00<00:00, 119MB/s]\u001b[A\n",
            "Downloading data:  48% 47.9M/99.7M [00:00<00:00, 122MB/s]\u001b[A\n",
            "Downloading data:  60% 60.1M/99.7M [00:00<00:00, 118MB/s]\u001b[A\n",
            "Downloading data:  73% 72.7M/99.7M [00:00<00:00, 120MB/s]\u001b[A\n",
            "Downloading data:  85% 84.7M/99.7M [00:00<00:00, 120MB/s]\u001b[A\n",
            "Downloading data: 100% 99.7M/99.7M [00:00<00:00, 120MB/s]\n",
            "Downloading data files: 100% 1/1 [00:01<00:00,  1.52s/it]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1489.45it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/lambdalabs___parquet/lambdalabs--pokemon-blip-captions-10e3527a764857bd/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
            "100% 1/1 [00:00<00:00, 260.81it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20230718_025041-r0vo84dr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msmart-water-4\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/zimo0706/text2image-fine-tune\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/zimo0706/text2image-fine-tune/runs/r0vo84dr\u001b[0m\n",
            "07/18/2023 02:50:42 - INFO - __main__ - ***** Running training *****\n",
            "07/18/2023 02:50:42 - INFO - __main__ -   Num examples = 833\n",
            "07/18/2023 02:50:42 - INFO - __main__ -   Num Epochs = 50\n",
            "07/18/2023 02:50:42 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
            "07/18/2023 02:50:42 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "07/18/2023 02:50:42 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
            "07/18/2023 02:50:42 - INFO - __main__ -   Total optimization steps = 41650\n",
            "Resuming from checkpoint checkpoint-25000\n",
            "07/18/2023 02:50:42 - INFO - accelerate.accelerator - Loading states from /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-25000\n",
            "07/18/2023 02:50:43 - INFO - accelerate.checkpointing - All model weights loaded successfully\n",
            "07/18/2023 02:50:44 - INFO - accelerate.checkpointing - All optimizer states loaded successfully\n",
            "07/18/2023 02:50:44 - INFO - accelerate.checkpointing - All scheduler states loaded successfully\n",
            "07/18/2023 02:50:44 - INFO - accelerate.checkpointing - GradScaler state loaded successfully\n",
            "07/18/2023 02:50:45 - INFO - accelerate.checkpointing - All random states loaded successfully\n",
            "07/18/2023 02:50:45 - INFO - accelerate.accelerator - Loading in 0 custom states\n",
            "Steps:   5% 833/16650 [06:02<1:53:06,  2.33it/s, lr=0.0001, step_loss=0.0937]07/18/2023 02:56:47 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "\n",
            "Downloading (…)ain/model_index.json: 100% 541/541 [00:00<00:00, 1.51MB/s]\n",
            "\n",
            "Fetching 14 files:   0% 0/14 [00:00<?, ?it/s]\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   0% 0.00/1.22G [00:00<?, ?B/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)nfig-checkpoint.json: 100% 209/209 [00:00<00:00, 443kB/s]\n",
            "\n",
            "\n",
            "\n",
            "Downloading (…)rocessor_config.json: 100% 342/342 [00:00<00:00, 1.18MB/s]\n",
            "\n",
            "Fetching 14 files:   7% 1/14 [00:00<00:03,  4.02it/s]\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   3% 31.5M/1.22G [00:00<00:04, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   6% 73.4M/1.22G [00:00<00:03, 350MB/s]\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "Downloading (…)_checker/config.json: 100% 4.56k/4.56k [00:00<00:00, 8.44MB/s]\n",
            "\n",
            "Fetching 14 files:  21% 3/14 [00:00<00:01,  7.82it/s]\u001b[A\n",
            "\n",
            "Downloading model.safetensors:   9% 115M/1.22G [00:00<00:03, 315MB/s] \u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  13% 157M/1.22G [00:00<00:04, 261MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  16% 189M/1.22G [00:00<00:03, 268MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  18% 220M/1.22G [00:00<00:03, 276MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  21% 252M/1.22G [00:00<00:03, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  23% 283M/1.22G [00:00<00:03, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  26% 315M/1.22G [00:01<00:03, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  28% 346M/1.22G [00:01<00:03, 286MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  31% 377M/1.22G [00:01<00:03, 269MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  34% 409M/1.22G [00:01<00:02, 272MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  36% 440M/1.22G [00:01<00:02, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  39% 472M/1.22G [00:01<00:02, 284MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  41% 503M/1.22G [00:01<00:02, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  44% 535M/1.22G [00:01<00:02, 280MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  47% 566M/1.22G [00:02<00:02, 271MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  50% 608M/1.22G [00:02<00:02, 285MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  53% 640M/1.22G [00:02<00:02, 282MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  55% 671M/1.22G [00:02<00:01, 281MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  58% 703M/1.22G [00:02<00:01, 289MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  60% 734M/1.22G [00:02<00:01, 292MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  63% 765M/1.22G [00:02<00:01, 294MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  66% 797M/1.22G [00:02<00:01, 273MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  68% 828M/1.22G [00:02<00:01, 258MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  71% 860M/1.22G [00:03<00:01, 257MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  73% 891M/1.22G [00:03<00:01, 255MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  76% 923M/1.22G [00:03<00:01, 238MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  78% 954M/1.22G [00:03<00:01, 227MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  81% 986M/1.22G [00:03<00:01, 206MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  84% 1.02G/1.22G [00:03<00:01, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  85% 1.04G/1.22G [00:04<00:00, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  87% 1.06G/1.22G [00:04<00:00, 189MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  89% 1.08G/1.22G [00:04<00:00, 193MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  91% 1.10G/1.22G [00:04<00:00, 190MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  93% 1.13G/1.22G [00:04<00:00, 210MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors:  96% 1.16G/1.22G [00:04<00:00, 230MB/s]\u001b[A\u001b[A\n",
            "\n",
            "Downloading model.safetensors: 100% 1.22G/1.22G [00:04<00:00, 254MB/s]\n",
            "\n",
            "Fetching 14 files: 100% 14/14 [00:04<00:00,  2.81it/s]\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:00<00:00,  8.56it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.59it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:03<00:01,  1.33it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:06<00:00,  1.04it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  10% 1666/16650 [12:59<1:48:07,  2.31it/s, lr=0.0001, step_loss=0.00382]07/18/2023 03:03:45 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.42it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  3.32it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:02<00:00,  3.01it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  15% 2499/16650 [19:45<1:42:19,  2.31it/s, lr=0.0001, step_loss=0.0325]07/18/2023 03:10:30 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.49it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.99it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.38it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  20% 3332/16650 [26:30<1:34:55,  2.34it/s, lr=0.0001, step_loss=0.0213] 07/18/2023 03:17:15 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  14% 1/7 [00:00<00:00,  9.38it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.20it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  3.27it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:02<00:00,  3.02it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  25% 4165/16650 [33:17<1:31:54,  2.26it/s, lr=0.0001, step_loss=0.00521]07/18/2023 03:24:02 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.45it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.95it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.40it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  30% 4998/16650 [40:02<1:23:37,  2.32it/s, lr=0.0001, step_loss=0.0579] 07/18/2023 03:30:47 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.43it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.93it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.33it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  30% 5010/16650 [40:40<2:06:48,  1.53it/s, lr=0.0001, step_loss=0.177]07/18/2023 03:31:25 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000\n",
            "07/18/2023 03:31:26 - INFO - accelerate.checkpointing - Model weights saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000/pytorch_model.bin\n",
            "07/18/2023 03:31:26 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000/optimizer.bin\n",
            "07/18/2023 03:31:26 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000/scheduler.bin\n",
            "07/18/2023 03:31:26 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000/scaler.pt\n",
            "07/18/2023 03:31:26 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000/random_states_0.pkl\n",
            "07/18/2023 03:31:26 - INFO - __main__ - Saved state to /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-30000\n",
            "Steps:  35% 5831/16650 [46:47<1:20:12,  2.25it/s, lr=0.0001, step_loss=0.0767]07/18/2023 03:37:32 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.72it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.09it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  3.81it/s]\n",
            "Steps:  40% 6664/16650 [53:33<1:11:56,  2.31it/s, lr=0.0001, step_loss=0.0164]07/18/2023 03:44:18 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.35it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.80it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.28it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  45% 7497/16650 [1:00:17<1:07:22,  2.26it/s, lr=0.0001, step_loss=0.071] 07/18/2023 03:51:03 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.48it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  3.44it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:02<00:00,  3.05it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  50% 8330/16650 [1:07:04<59:23,  2.34it/s, lr=0.0001, step_loss=0.00832]07/18/2023 03:57:50 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.43it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.89it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.31it/s]\n",
            "Steps:  55% 9163/16650 [1:13:49<53:22,  2.34it/s, lr=0.0001, step_loss=0.0839] 07/18/2023 04:04:35 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.48it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.69it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  3.78it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  60% 9996/16650 [1:20:35<48:37,  2.28it/s, lr=0.0001, step_loss=0.00446]07/18/2023 04:11:20 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.38it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  3.31it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:02<00:00,  3.01it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  60% 10010/16650 [1:21:15<59:28,  1.86it/s, lr=0.0001, step_loss=0.0513]  07/18/2023 04:12:01 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000\n",
            "07/18/2023 04:12:01 - INFO - accelerate.checkpointing - Model weights saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000/pytorch_model.bin\n",
            "07/18/2023 04:12:01 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000/optimizer.bin\n",
            "07/18/2023 04:12:01 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000/scheduler.bin\n",
            "07/18/2023 04:12:01 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000/scaler.pt\n",
            "07/18/2023 04:12:01 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000/random_states_0.pkl\n",
            "07/18/2023 04:12:01 - INFO - __main__ - Saved state to /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-35000\n",
            "Steps:  65% 10829/16650 [1:27:21<42:03,  2.31it/s, lr=0.0001, step_loss=0.0827]07/18/2023 04:18:07 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.43it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.88it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.25it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  70% 11662/16650 [1:34:07<35:40,  2.33it/s, lr=0.0001, step_loss=0.0912]07/18/2023 04:24:52 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.08it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.10it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  3.53it/s]\n",
            "Potential NSFW content was detected in one or more images. A black image will be returned instead. Try again with a different prompt and/or seed.\n",
            "Steps:  75% 12495/16650 [1:40:54<30:58,  2.24it/s, lr=0.0001, step_loss=0.0404]07/18/2023 04:31:39 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.33it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  3.27it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:02<00:00,  3.09it/s]\n",
            "Steps:  80% 13328/16650 [1:47:40<23:52,  2.32it/s, lr=0.0001, step_loss=0.0321]07/18/2023 04:38:25 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.40it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.85it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.31it/s]\n",
            "Steps:  85% 14161/16650 [1:54:25<17:56,  2.31it/s, lr=0.0001, step_loss=0.0258]07/18/2023 04:45:11 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:01<00:01,  2.55it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  3.55it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:02<00:00,  3.20it/s]\n",
            "Steps:  90% 14994/16650 [2:01:13<12:27,  2.22it/s, lr=0.0001, step_loss=0.00497]07/18/2023 04:51:58 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.24it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.57it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.11it/s]\n",
            "Steps:  90% 15010/16650 [2:01:53<13:18,  2.05it/s, lr=0.0001, step_loss=0.00761]07/18/2023 04:52:39 - INFO - accelerate.accelerator - Saving current state to /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000\n",
            "07/18/2023 04:52:39 - INFO - accelerate.checkpointing - Model weights saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000/pytorch_model.bin\n",
            "07/18/2023 04:52:39 - INFO - accelerate.checkpointing - Optimizer state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000/optimizer.bin\n",
            "07/18/2023 04:52:39 - INFO - accelerate.checkpointing - Scheduler state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000/scheduler.bin\n",
            "07/18/2023 04:52:39 - INFO - accelerate.checkpointing - Gradient scaler state saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000/scaler.pt\n",
            "07/18/2023 04:52:39 - INFO - accelerate.checkpointing - Random states saved in /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000/random_states_0.pkl\n",
            "07/18/2023 04:52:39 - INFO - __main__ - Saved state to /content/drive/MyDrive/sd-pokemon-model-lora/checkpoint-40000\n",
            "Steps:  95% 15827/16650 [2:08:00<05:55,  2.32it/s, lr=0.0001, step_loss=0.0313] 07/18/2023 04:58:45 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.33it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.76it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.22it/s]\n",
            "Steps: : 16660it [2:14:46,  2.31it/s, lr=0.0001, step_loss=0.127] 07/18/2023 05:05:32 - INFO - __main__ - Running validation... \n",
            " Generating 4 images with prompt: cute dragon creature.\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.39it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:01<00:00,  4.83it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:01<00:00,  4.23it/s]\n",
            "Model weights saved in /content/drive/MyDrive/sd-pokemon-model-lora/pytorch_lora_weights.bin\n",
            "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
            "\n",
            "Loading pipeline components...:   0% 0/7 [00:00<?, ?it/s]\u001b[ALoaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "Loaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
            "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  43% 3/7 [00:00<00:01,  3.86it/s]\u001b[A{'prediction_type', 'timestep_spacing'} was not found in config. Values will be initialized to default values.\n",
            "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "{'norm_num_groups', 'force_upcast'} was not found in config. Values will be initialized to default values.\n",
            "Loaded vae as AutoencoderKL from `vae` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  71% 5/7 [00:00<00:00,  5.62it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...:  86% 6/7 [00:01<00:00,  3.87it/s]\u001b[A{'class_embeddings_concat', 'only_cross_attention', 'projection_class_embeddings_input_dim', 'upcast_attention', 'mid_block_type', 'num_attention_heads', 'encoder_hid_dim_type', 'addition_embed_type_num_heads', 'num_class_embeds', 'time_embedding_act_fn', 'dual_cross_attention', 'class_embed_type', 'time_embedding_type', 'mid_block_only_cross_attention', 'conv_in_kernel', 'encoder_hid_dim', 'use_linear_projection', 'cross_attention_norm', 'resnet_time_scale_shift', 'resnet_out_scale_factor', 'addition_embed_type', 'time_embedding_dim', 'conv_out_kernel', 'transformer_layers_per_block', 'resnet_skip_time_act', 'addition_time_embed_dim', 'timestep_post_act', 'time_cond_proj_dim'} was not found in config. Values will be initialized to default values.\n",
            "Loaded unet as UNet2DConditionModel from `unet` subfolder of CompVis/stable-diffusion-v1-4.\n",
            "\n",
            "Loading pipeline components...: 100% 7/7 [00:21<00:00,  3.03s/it]\n",
            "\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/30 [00:00<00:10,  2.67it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:06,  4.05it/s]\u001b[A\n",
            " 10% 3/30 [00:00<00:05,  4.82it/s]\u001b[A\n",
            " 13% 4/30 [00:00<00:04,  5.29it/s]\u001b[A\n",
            " 17% 5/30 [00:01<00:04,  5.58it/s]\u001b[A\n",
            " 20% 6/30 [00:01<00:04,  5.71it/s]\u001b[A\n",
            " 23% 7/30 [00:01<00:03,  5.86it/s]\u001b[A\n",
            " 27% 8/30 [00:01<00:03,  5.97it/s]\u001b[A\n",
            " 30% 9/30 [00:01<00:03,  6.04it/s]\u001b[A\n",
            " 33% 10/30 [00:01<00:03,  6.10it/s]\u001b[A\n",
            " 37% 11/30 [00:01<00:03,  6.14it/s]\u001b[A\n",
            " 40% 12/30 [00:02<00:02,  6.12it/s]\u001b[A\n",
            " 43% 13/30 [00:02<00:02,  6.10it/s]\u001b[A\n",
            " 47% 14/30 [00:02<00:02,  6.12it/s]\u001b[A\n",
            " 50% 15/30 [00:02<00:02,  6.16it/s]\u001b[A\n",
            " 53% 16/30 [00:02<00:02,  6.16it/s]\u001b[A\n",
            " 57% 17/30 [00:02<00:02,  6.17it/s]\u001b[A\n",
            " 60% 18/30 [00:03<00:01,  6.15it/s]\u001b[A\n",
            " 63% 19/30 [00:03<00:01,  6.13it/s]\u001b[A\n",
            " 67% 20/30 [00:03<00:01,  6.13it/s]\u001b[A\n",
            " 70% 21/30 [00:03<00:01,  6.14it/s]\u001b[A\n",
            " 73% 22/30 [00:03<00:01,  6.17it/s]\u001b[A\n",
            " 77% 23/30 [00:03<00:01,  6.19it/s]\u001b[A\n",
            " 80% 24/30 [00:04<00:00,  6.19it/s]\u001b[A\n",
            " 83% 25/30 [00:04<00:00,  6.18it/s]\u001b[A\n",
            " 87% 26/30 [00:04<00:00,  6.15it/s]\u001b[A\n",
            " 90% 27/30 [00:04<00:00,  6.15it/s]\u001b[A\n",
            " 93% 28/30 [00:04<00:00,  6.15it/s]\u001b[A\n",
            " 97% 29/30 [00:04<00:00,  6.16it/s]\u001b[A\n",
            "100% 30/30 [00:05<00:00,  5.91it/s]\n",
            "\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/30 [00:00<00:09,  3.20it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:06,  4.48it/s]\u001b[A\n",
            " 10% 3/30 [00:00<00:05,  5.11it/s]\u001b[A\n",
            " 13% 4/30 [00:00<00:04,  5.49it/s]\u001b[A\n",
            " 17% 5/30 [00:00<00:04,  5.61it/s]\u001b[A\n",
            " 20% 6/30 [00:01<00:04,  5.77it/s]\u001b[A\n",
            " 23% 7/30 [00:01<00:03,  5.87it/s]\u001b[A\n",
            " 27% 8/30 [00:01<00:03,  5.95it/s]\u001b[A\n",
            " 30% 9/30 [00:01<00:03,  5.97it/s]\u001b[A\n",
            " 33% 10/30 [00:01<00:03,  6.01it/s]\u001b[A\n",
            " 37% 11/30 [00:01<00:03,  6.02it/s]\u001b[A\n",
            " 40% 12/30 [00:02<00:02,  6.04it/s]\u001b[A\n",
            " 43% 13/30 [00:02<00:02,  6.04it/s]\u001b[A\n",
            " 47% 14/30 [00:02<00:02,  6.04it/s]\u001b[A\n",
            " 50% 15/30 [00:02<00:02,  6.04it/s]\u001b[A\n",
            " 53% 16/30 [00:02<00:02,  6.10it/s]\u001b[A\n",
            " 57% 17/30 [00:02<00:02,  6.05it/s]\u001b[A\n",
            " 60% 18/30 [00:03<00:01,  6.07it/s]\u001b[A\n",
            " 63% 19/30 [00:03<00:01,  6.08it/s]\u001b[A\n",
            " 67% 20/30 [00:03<00:01,  6.09it/s]\u001b[A\n",
            " 70% 21/30 [00:03<00:01,  6.06it/s]\u001b[A\n",
            " 73% 22/30 [00:03<00:01,  6.06it/s]\u001b[A\n",
            " 77% 23/30 [00:03<00:01,  6.02it/s]\u001b[A\n",
            " 80% 24/30 [00:04<00:00,  6.04it/s]\u001b[A\n",
            " 83% 25/30 [00:04<00:00,  6.06it/s]\u001b[A\n",
            " 87% 26/30 [00:04<00:00,  6.08it/s]\u001b[A\n",
            " 90% 27/30 [00:04<00:00,  6.07it/s]\u001b[A\n",
            " 93% 28/30 [00:04<00:00,  6.04it/s]\u001b[A\n",
            " 97% 29/30 [00:04<00:00,  6.02it/s]\u001b[A\n",
            "100% 30/30 [00:05<00:00,  5.89it/s]\n",
            "\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/30 [00:00<00:09,  3.17it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:06,  4.42it/s]\u001b[A\n",
            " 10% 3/30 [00:00<00:05,  5.09it/s]\u001b[A\n",
            " 13% 4/30 [00:00<00:04,  5.46it/s]\u001b[A\n",
            " 17% 5/30 [00:00<00:04,  5.48it/s]\u001b[A\n",
            " 20% 6/30 [00:01<00:04,  5.69it/s]\u001b[A\n",
            " 23% 7/30 [00:01<00:03,  5.84it/s]\u001b[A\n",
            " 27% 8/30 [00:01<00:03,  5.94it/s]\u001b[A\n",
            " 30% 9/30 [00:01<00:03,  6.01it/s]\u001b[A\n",
            " 33% 10/30 [00:01<00:03,  6.06it/s]\u001b[A\n",
            " 37% 11/30 [00:01<00:03,  5.96it/s]\u001b[A\n",
            " 40% 12/30 [00:02<00:03,  5.94it/s]\u001b[A\n",
            " 43% 13/30 [00:02<00:02,  5.97it/s]\u001b[A\n",
            " 47% 14/30 [00:02<00:02,  5.96it/s]\u001b[A\n",
            " 50% 15/30 [00:02<00:02,  5.96it/s]\u001b[A\n",
            " 53% 16/30 [00:02<00:02,  5.97it/s]\u001b[A\n",
            " 57% 17/30 [00:02<00:02,  5.95it/s]\u001b[A\n",
            " 60% 18/30 [00:03<00:02,  5.91it/s]\u001b[A\n",
            " 63% 19/30 [00:03<00:01,  5.95it/s]\u001b[A\n",
            " 67% 20/30 [00:03<00:01,  5.95it/s]\u001b[A\n",
            " 70% 21/30 [00:03<00:01,  5.97it/s]\u001b[A\n",
            " 73% 22/30 [00:03<00:01,  5.98it/s]\u001b[A\n",
            " 77% 23/30 [00:03<00:01,  5.98it/s]\u001b[A\n",
            " 80% 24/30 [00:04<00:01,  5.96it/s]\u001b[A\n",
            " 83% 25/30 [00:04<00:00,  5.93it/s]\u001b[A\n",
            " 87% 26/30 [00:04<00:00,  5.95it/s]\u001b[A\n",
            " 90% 27/30 [00:04<00:00,  5.92it/s]\u001b[A\n",
            " 93% 28/30 [00:04<00:00,  5.91it/s]\u001b[A\n",
            " 97% 29/30 [00:04<00:00,  5.93it/s]\u001b[A\n",
            "100% 30/30 [00:05<00:00,  5.81it/s]\n",
            "\n",
            "  0% 0/30 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 1/30 [00:00<00:09,  3.07it/s]\u001b[A\n",
            "  7% 2/30 [00:00<00:06,  4.31it/s]\u001b[A\n",
            " 10% 3/30 [00:00<00:05,  4.93it/s]\u001b[A\n",
            " 13% 4/30 [00:00<00:04,  5.23it/s]\u001b[A\n",
            " 17% 5/30 [00:01<00:04,  5.35it/s]\u001b[A\n",
            " 20% 6/30 [00:01<00:04,  5.55it/s]\u001b[A\n",
            " 23% 7/30 [00:01<00:04,  5.63it/s]\u001b[A\n",
            " 27% 8/30 [00:01<00:03,  5.73it/s]\u001b[A\n",
            " 30% 9/30 [00:01<00:03,  5.79it/s]\u001b[A\n",
            " 33% 10/30 [00:01<00:03,  5.77it/s]\u001b[A\n",
            " 37% 11/30 [00:02<00:03,  5.81it/s]\u001b[A\n",
            " 40% 12/30 [00:02<00:03,  5.82it/s]\u001b[A\n",
            " 43% 13/30 [00:02<00:02,  5.84it/s]\u001b[A\n",
            " 47% 14/30 [00:02<00:02,  5.87it/s]\u001b[A\n",
            " 50% 15/30 [00:02<00:02,  5.85it/s]\u001b[A\n",
            " 53% 16/30 [00:02<00:02,  5.81it/s]\u001b[A\n",
            " 57% 17/30 [00:03<00:02,  5.82it/s]\u001b[A\n",
            " 60% 18/30 [00:03<00:02,  5.79it/s]\u001b[A\n",
            " 63% 19/30 [00:03<00:01,  5.81it/s]\u001b[A\n",
            " 67% 20/30 [00:03<00:01,  5.85it/s]\u001b[A\n",
            " 70% 21/30 [00:03<00:01,  5.83it/s]\u001b[A\n",
            " 73% 22/30 [00:03<00:01,  5.85it/s]\u001b[A\n",
            " 77% 23/30 [00:04<00:01,  5.86it/s]\u001b[A\n",
            " 80% 24/30 [00:04<00:01,  5.83it/s]\u001b[A\n",
            " 83% 25/30 [00:04<00:00,  5.82it/s]\u001b[A\n",
            " 87% 26/30 [00:04<00:00,  5.84it/s]\u001b[A\n",
            " 90% 27/30 [00:04<00:00,  5.85it/s]\u001b[A\n",
            " 93% 28/30 [00:04<00:00,  5.84it/s]\u001b[A\n",
            " 97% 29/30 [00:05<00:00,  5.82it/s]\u001b[A\n",
            "100% 30/30 [00:05<00:00,  5.67it/s]\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss ▁▆▂▃▃▂▁▁▂▄▁▃▁▅▂▂▂▁▁▃▁█▃▅▂▁▁▄▅▅▁█▁▃▃▁▂▁▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train_loss 0.12728\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msmart-water-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/zimo0706/text2image-fine-tune/runs/r0vo84dr\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 83 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230718_025041-r0vo84dr/logs\u001b[0m\n",
            "/usr/local/lib/python3.10/dist-packages/wandb/sdk/wandb_run.py:2082: UserWarning: Run (r0vo84dr) is finished. The call to `_console_raw_callback` will be ignored. Please make sure that you are using an active run.\n",
            "  lambda data: self._console_raw_callback(\"stderr\", data),\n",
            "Steps: : 16660it [2:16:13,  2.04it/s, lr=0.0001, step_loss=0.127]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VnmqQ2eZb73-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/sd-pokemon-model-lora\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
        "pipe.unet.load_attn_procs(model_path)\n",
        "pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"A pokemon with green eyes and red legs.\"\n",
        "image = pipe(prompt, num_inference_steps=30, guidance_scale=7.5).images[0]\n",
        "image.save(\"/content/drive/MyDrive/pokemon.png\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "8537bd04d550426da2a24eb652278814",
            "0c3a0244006c43d4826feef84774616d",
            "ddccfe049ed846a793536909b291b02d",
            "ffdb291e7b71439ea5ce9681f23fc68b",
            "2a0f54134f94400a8d655b9105c9e937",
            "77cc285cf24f4bfc881dbaa2ae61ccfe",
            "e0455fdf3c7b464e9e895f12007d567a",
            "2812ded5de7a434a934acaca6849e772",
            "93f95b4f79564da9b9635cc9639b61c0",
            "42b3fc1b492745dab67832292e6f7942",
            "048fcf2adb77493686f936b608e6ac0f",
            "97c7bd5cb61e40c2b3e0d986e68154d7",
            "c305d5aa9bcf4bd396cadb5e828fa063",
            "39529781bc084509b54f6910c4ed3a2d",
            "b33deda178e446ffa239a3b6ed3b38af",
            "813d85bb2bbb414d806fb01a791b1bd3",
            "549afad2ec3b475baff0c1c26e42b991",
            "b6bb244e8e9e4458a45a40bb3ba5a22d",
            "9d4177cfeaa148feb0a11a03b8b871b8",
            "3c796bfb7901469fa34de2d826828f34",
            "a24483bc2be24dd3b94704b8afaab39e",
            "b0dbb5dab30d45e98964baefda505719"
          ]
        },
        "id": "PI-GUOyPZdUX",
        "outputId": "da0d6464-f074-4faa-b21e-4b3c729df293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading pipeline components...:   0%|          | 0/7 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8537bd04d550426da2a24eb652278814"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97c7bd5cb61e40c2b3e0d986e68154d7"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install git+https://github.com/cloneofsimo/lora.git"
      ],
      "metadata": {
        "id": "fACkoe7VBKae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "\n",
        "model_id = \"dreamlike-art/dreamlike-photoreal-2.0\"\n",
        "pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "\n",
        "prompt = \"photo, a church in the middle of a field of crops, bright cinematic lighting, gopro, fisheye lens\"\n",
        "image = pipe(prompt).images[0]\n",
        "image"
      ],
      "metadata": {
        "id": "UEh7C-tj6t0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ! pip install accelerate"
      ],
      "metadata": {
        "id": "uN8pH_jYEedC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! sudo lora_pti \\\n",
        "  --pretrained_model_name_or_path=\"runwayml/stable-diffusion-v1-5\"  \\\n",
        "  --instance_data_dir=\"./content/drive/MyDrive/summer2023/task4/data_disney\" \\\n",
        "  --output_dir=\"./content/drive/MyDrive/summer2023/task4/output_dsn\" \\\n",
        "  --train_text_encoder \\\n",
        "  --resolution=512 \\\n",
        "  --train_batch_size=1 \\\n",
        "  --gradient_accumulation_steps=4 \\\n",
        "  --scale_lr \\\n",
        "  --learning_rate_unet=1e-4 \\\n",
        "  --learning_rate_text=1e-5 \\\n",
        "  --learning_rate_ti=5e-4 \\\n",
        "  --color_jitter \\\n",
        "  --lr_scheduler=\"linear\" \\\n",
        "  --lr_warmup_steps=0 \\\n",
        "  --placeholder_tokens=\"<s1>|<s2>\" \\\n",
        "  --use_template=\"style\"\\\n",
        "  --save_steps=100 \\\n",
        "  --max_train_steps_ti=1000 \\\n",
        "  --max_train_steps_tuning=1000 \\\n",
        "  --perform_inversion=True \\\n",
        "  --clip_ti_decay \\\n",
        "  --weight_decay_ti=0.000 \\\n",
        "  --weight_decay_lora=0.001\\\n",
        "  --continue_inversion \\\n",
        "  --continue_inversion_lr=1e-4 \\\n",
        "  --device=\"cuda:0\" \\\n",
        "  --lora_rank=1 \\\n",
        "#  --use_face_segmentation_condition\\"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-BaBjPWBbZu",
        "outputId": "8a53c9db-255b-41ab-808e-d2fb84f404e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-13 12:58:09.635983: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "PTI : Initializer Tokens not given, doing random inits\n",
            "PTI : Placeholder Tokens ['<s1>', '<s2>']\n",
            "PTI : Initializer Tokens ['<rand-0.017>', '<rand-0.017>']\n",
            "Downloading (…)tokenizer/vocab.json: 100% 1.06M/1.06M [00:00<00:00, 5.02MB/s]\n",
            "Downloading (…)tokenizer/merges.txt: 100% 525k/525k [00:00<00:00, 71.0MB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 472/472 [00:00<00:00, 3.09MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 806/806 [00:00<00:00, 4.99MB/s]\n",
            "Downloading (…)_encoder/config.json: 100% 617/617 [00:00<00:00, 3.94MB/s]\n",
            "Downloading model.safetensors: 100% 492M/492M [00:01<00:00, 276MB/s]\n",
            "Initialized <s1> with random noise (sigma=0.017), empirically 0.001 +- 0.018\n",
            "Norm : 0.4863\n",
            "Initialized <s2> with random noise (sigma=0.017), empirically 0.000 +- 0.017\n",
            "Norm : 0.4781\n",
            "Downloading (…)main/vae/config.json: 100% 547/547 [00:00<00:00, 3.56MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 335M/335M [00:00<00:00, 355MB/s]\n",
            "Downloading (…)ain/unet/config.json: 100% 743/743 [00:00<00:00, 3.51MB/s]\n",
            "Downloading (…)ch_model.safetensors: 100% 3.44G/3.44G [00:27<00:00, 123MB/s] \n",
            "\u001b[31m╭─\u001b[0m\u001b[31m────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m─────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/bin/\u001b[0m\u001b[1;33mlora_pti\u001b[0m:\u001b[94m8\u001b[0m in \u001b[92m<module>\u001b[0m                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m5 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mlora_diffusion\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mcli_lora_pti\u001b[0m \u001b[94mimport\u001b[0m main                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m6 \u001b[0m\u001b[94mif\u001b[0m \u001b[91m__name__\u001b[0m == \u001b[33m'\u001b[0m\u001b[33m__main__\u001b[0m\u001b[33m'\u001b[0m:                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m7 \u001b[0m\u001b[2m│   \u001b[0msys.argv[\u001b[94m0\u001b[0m] = re.sub(\u001b[33mr\u001b[0m\u001b[33m'\u001b[0m\u001b[33m(-script\u001b[0m\u001b[33m\\\u001b[0m\u001b[33m.pyw|\u001b[0m\u001b[33m\\\u001b[0m\u001b[33m.exe)?$\u001b[0m\u001b[33m'\u001b[0m, \u001b[33m'\u001b[0m\u001b[33m'\u001b[0m, sys.argv[\u001b[94m0\u001b[0m])     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m8 \u001b[2m│   \u001b[0msys.exit(main())                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m9 \u001b[0m                                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/lora_diffusion/\u001b[0m\u001b[1;33mcli_lora_pti.py\u001b[0m:\u001b[94m1040\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mmain\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1037 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1038 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1039 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mmain\u001b[0m():                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1040 \u001b[2m│   \u001b[0mfire.Fire(train)                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1041 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m141\u001b[0m in \u001b[92mFire\u001b[0m             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m138 \u001b[0m\u001b[2m│   \u001b[0mcontext.update(caller_globals)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2m│   \u001b[0mcontext.update(caller_locals)                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m141 \u001b[2m  \u001b[0mcomponent_trace = _Fire(component, args, parsed_flag_args, context,  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m142 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mif\u001b[0m component_trace.HasError():                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   \u001b[0m_DisplayError(component_trace)                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m475\u001b[0m in \u001b[92m_Fire\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m472 \u001b[0m\u001b[2m│     \u001b[0mis_class = inspect.isclass(component)                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m473 \u001b[0m\u001b[2m│     \u001b[0m                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m474 \u001b[0m\u001b[2m│     \u001b[0m\u001b[94mtry\u001b[0m:                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m475 \u001b[2m│   │   \u001b[0mcomponent, remaining_args = _CallAndUpdateTrace(               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m476 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomponent,                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m477 \u001b[0m\u001b[2m│   │   │   \u001b[0mremaining_args,                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m478 \u001b[0m\u001b[2m│   │   │   \u001b[0mcomponent_trace,                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/fire/\u001b[0m\u001b[1;33mcore.py\u001b[0m:\u001b[94m691\u001b[0m in                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_CallAndUpdateTrace\u001b[0m                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m688 \u001b[0m\u001b[2m│   \u001b[0mloop = asyncio.get_event_loop()                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m689 \u001b[0m\u001b[2m│   \u001b[0mcomponent = loop.run_until_complete(fn(*varargs, **kwargs))        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m690 \u001b[0m\u001b[2m  \u001b[0m\u001b[94melse\u001b[0m:                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m691 \u001b[2m│   \u001b[0mcomponent = fn(*varargs, **kwargs)                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m692 \u001b[0m\u001b[2m  \u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m693 \u001b[0m\u001b[2m  \u001b[0m\u001b[94mif\u001b[0m treatment == \u001b[33m'\u001b[0m\u001b[33mclass\u001b[0m\u001b[33m'\u001b[0m:                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m694 \u001b[0m\u001b[2m│   \u001b[0maction = trace.INSTANTIATED_CLASS                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/lora_diffusion/\u001b[0m\u001b[1;33mcli_lora_pti.py\u001b[0m:\u001b[94m804\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mtrain\u001b[0m                                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 801 \u001b[0m\u001b[2m│   \u001b[0m\u001b[96mprint\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mPTI : Initializer Tokens\u001b[0m\u001b[33m\"\u001b[0m, initializer_tokens)             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 802 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 803 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# get the models\u001b[0m                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 804 \u001b[2m│   \u001b[0mtext_encoder, vae, unet, tokenizer, placeholder_token_ids = get_m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 805 \u001b[0m\u001b[2m│   │   \u001b[0mpretrained_model_name_or_path,                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 806 \u001b[0m\u001b[2m│   │   \u001b[0mpretrained_vae_name_or_path,                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 807 \u001b[0m\u001b[2m│   │   \u001b[0mrevision,                                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/lora_diffusion/\u001b[0m\u001b[1;33mcli_lora_pti.py\u001b[0m:\u001b[94m123\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mget_models\u001b[0m                                                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 120 \u001b[0m\u001b[2m│   \u001b[0m)                                                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 121 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 122 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m (                                                          \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 123 \u001b[2m│   │   \u001b[0mtext_encoder.to(device),                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 124 \u001b[0m\u001b[2m│   │   \u001b[0mvae.to(device),                                               \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 125 \u001b[0m\u001b[2m│   │   \u001b[0munet.to(device),                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 126 \u001b[0m\u001b[2m│   │   \u001b[0mtokenizer,                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/transformers/\u001b[0m\u001b[1;33mmodeling_utils.py\u001b[0m:\u001b[94m1902\u001b[0m  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m in \u001b[92mto\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1899 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33m model has already been set to the correct devices a\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1900 \u001b[0m\u001b[2m│   │   │   \u001b[0m)                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1901 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1902 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96msuper\u001b[0m().to(*args, **kwargs)                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1903 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1904 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mhalf\u001b[0m(\u001b[96mself\u001b[0m, *args):                                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1905 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Checks if the model has been loaded in 8-bit\u001b[0m                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1145\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mto\u001b[0m                                                                           \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1142 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mnon_blocking, memory_format=convert_to_fo \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1143 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.i \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1145 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._apply(convert)                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1146 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1147 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mregister_full_backward_pre_hook\u001b[0m(                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1148 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m,                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m797\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_apply\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 794 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 795 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn):                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 796 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 797 \u001b[2m│   │   │   \u001b[0mmodule._apply(fn)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 798 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 799 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 800 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m797\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_apply\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 794 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 795 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn):                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 796 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 797 \u001b[2m│   │   │   \u001b[0mmodule._apply(fn)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 798 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 799 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 800 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m797\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_apply\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 794 \u001b[0m\u001b[2m│   \u001b[0m                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 795 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92m_apply\u001b[0m(\u001b[96mself\u001b[0m, fn):                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 796 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mfor\u001b[0m module \u001b[95min\u001b[0m \u001b[96mself\u001b[0m.children():                                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 797 \u001b[2m│   │   │   \u001b[0mmodule._apply(fn)                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 798 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 799 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mcompute_should_use_set_data\u001b[0m(tensor, tensor_applied):      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 800 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m torch._has_compatible_shallow_copy_type(tensor, tensor \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m820\u001b[0m in    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_apply\u001b[0m                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 817 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# track autograd history of `param_applied`, so we have t\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 818 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[2m# `with torch.no_grad():`\u001b[0m                                 \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 819 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mwith\u001b[0m torch.no_grad():                                     \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 820 \u001b[2m│   │   │   │   \u001b[0mparam_applied = fn(param)                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 821 \u001b[0m\u001b[2m│   │   │   \u001b[0mshould_use_set_data = compute_should_use_set_data(param,  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 822 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m should_use_set_data:                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 823 \u001b[0m\u001b[2m│   │   │   │   \u001b[0mparam.data = param_applied                            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1143\u001b[0m in   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92mconvert\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1140 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mif\u001b[0m convert_to_format \u001b[95mis\u001b[0m \u001b[95mnot\u001b[0m \u001b[94mNone\u001b[0m \u001b[95mand\u001b[0m t.dim() \u001b[95min\u001b[0m (\u001b[94m4\u001b[0m, \u001b[94m5\u001b[0m):   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1141 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1142 \u001b[0m\u001b[2m│   │   │   │   │   │   │   \u001b[0mnon_blocking, memory_format=convert_to_fo \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1143 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m t.to(device, dtype \u001b[94mif\u001b[0m t.is_floating_point() \u001b[95mor\u001b[0m t.i \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1144 \u001b[0m\u001b[2m│   │   \u001b[0m                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1145 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[96mself\u001b[0m._apply(convert)                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m1146 \u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/torch/cuda/\u001b[0m\u001b[1;33m__init__.py\u001b[0m:\u001b[94m247\u001b[0m in        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92m_lazy_init\u001b[0m                                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                              \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 244 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# are found or any other error occurs\u001b[0m                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 245 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m \u001b[95mnot\u001b[0m \u001b[95min\u001b[0m os.environ:                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 246 \u001b[0m\u001b[2m│   │   │   \u001b[0mos.environ[\u001b[33m'\u001b[0m\u001b[33mCUDA_MODULE_LOADING\u001b[0m\u001b[33m'\u001b[0m] = \u001b[33m'\u001b[0m\u001b[33mLAZY\u001b[0m\u001b[33m'\u001b[0m                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 247 \u001b[2m│   │   \u001b[0mtorch._C._cuda_init()                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 248 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 249 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# we need to just return without initializing in that case.\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 250 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# However, we must not let any *other* threads in!\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mRuntimeError: \u001b[0mFound no NVIDIA driver on your system. Please check that you have \n",
            "an NVIDIA GPU and installed a driver from \n",
            "\u001b[4;94mhttp://www.nvidia.com/Download/index.aspx\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod +x /content/drive/MyDrive/summer2023/task4/use_face_conditioning_example.sh"
      ],
      "metadata": {
        "id": "K3FOMmBXOI_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! /content/drive/MyDrive/summer2023/task4/use_face_conditioning_example.sh"
      ],
      "metadata": {
        "id": "A8LUcc6mBkge"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__, torch.cuda.is_available())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jmd3O-GZJWFi",
        "outputId": "8d3295c9-da89-4687-994b-03399b193f70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118 True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/huggingface/diffusers\n",
        "! cd diffusers\n",
        "! pip install ."
      ],
      "metadata": {
        "id": "YaZa0GCbJ1Nh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5226585-a2c9-4e51-b15d-ed8a369b7db2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'diffusers'...\n",
            "remote: Enumerating objects: 29887, done.\u001b[K\n",
            "remote: Counting objects: 100% (1846/1846), done.\u001b[K\n",
            "remote: Compressing objects: 100% (772/772), done.\u001b[K\n",
            "remote: Total 29887 (delta 1252), reused 1416 (delta 937), pack-reused 28041\u001b[K\n",
            "Receiving objects: 100% (29887/29887), 19.86 MiB | 20.00 MiB/s, done.\n",
            "Resolving deltas: 100% (21797/21797), done.\n",
            "\u001b[31mERROR: Directory '.' is not installable. Neither 'setup.py' nor 'pyproject.toml' found.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! cd /content/diffusers/examples"
      ],
      "metadata": {
        "id": "6_WU8cZfjYgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r /content/diffusers/examples/text_to_image/requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_RKBjyHj2Kt",
        "outputId": "7318af7a-4003-408b-9c0a-6c65840c84c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Installing collected packages: tokenizers, safetensors, xxhash, ftfy, dill, multiprocess, huggingface-hub, transformers, datasets, accelerate\n",
            "Successfully installed accelerate-0.20.3 datasets-2.13.1 dill-0.3.6 ftfy-6.1.1 huggingface-hub-0.16.4 multiprocess-0.70.14 safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2 xxhash-3.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! accelerate launch --config_file /content/config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZeFLYwPUj37s",
        "outputId": "001dad23-a55f-4366-f1a4-ace4d01a52b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-13 14:12:27.791260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "usage: accelerate <command> [<args>] launch\n",
            "       [-h]\n",
            "       [--config_file CONFIG_FILE]\n",
            "       [--quiet]\n",
            "       [--cpu]\n",
            "       [--multi_gpu]\n",
            "       [--tpu]\n",
            "       [--ipex]\n",
            "       [--mixed_precision {no,fp16,bf16,fp8}]\n",
            "       [--num_processes NUM_PROCESSES]\n",
            "       [--num_machines NUM_MACHINES]\n",
            "       [--num_cpu_threads_per_process NUM_CPU_THREADS_PER_PROCESS]\n",
            "       [--dynamo_backend {no,eager,aot_eager,inductor,nvfuser,aot_nvfuser,aot_cudagraphs,ofi,fx2trt,onnxrt,ipex}]\n",
            "       [--dynamo_mode {default,reduce-overhead,max-autotune}]\n",
            "       [--dynamo_use_fullgraph]\n",
            "       [--dynamo_use_dynamic]\n",
            "       [--use_deepspeed]\n",
            "       [--use_fsdp]\n",
            "       [--use_megatron_lm]\n",
            "       [--use_xpu]\n",
            "       [--gpu_ids GPU_IDS]\n",
            "       [--same_network]\n",
            "       [--machine_rank MACHINE_RANK]\n",
            "       [--main_process_ip MAIN_PROCESS_IP]\n",
            "       [--main_process_port MAIN_PROCESS_PORT]\n",
            "       [-t TEE]\n",
            "       [--role ROLE]\n",
            "       [--rdzv_backend RDZV_BACKEND]\n",
            "       [--rdzv_conf RDZV_CONF]\n",
            "       [--max_restarts MAX_RESTARTS]\n",
            "       [--monitor_interval MONITOR_INTERVAL]\n",
            "       [-m]\n",
            "       [--no_python]\n",
            "       [--tpu_cluster]\n",
            "       [--no_tpu_cluster]\n",
            "       [--tpu_use_sudo]\n",
            "       [--vm VM]\n",
            "       [--env ENV]\n",
            "       [--main_training_function MAIN_TRAINING_FUNCTION]\n",
            "       [--downcast_bf16]\n",
            "       [--deepspeed_config_file DEEPSPEED_CONFIG_FILE]\n",
            "       [--zero_stage ZERO_STAGE]\n",
            "       [--offload_optimizer_device OFFLOAD_OPTIMIZER_DEVICE]\n",
            "       [--offload_param_device OFFLOAD_PARAM_DEVICE]\n",
            "       [--offload_optimizer_nvme_path OFFLOAD_OPTIMIZER_NVME_PATH]\n",
            "       [--offload_param_nvme_path OFFLOAD_PARAM_NVME_PATH]\n",
            "       [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
            "       [--gradient_clipping GRADIENT_CLIPPING]\n",
            "       [--zero3_init_flag ZERO3_INIT_FLAG]\n",
            "       [--zero3_save_16bit_model ZERO3_SAVE_16BIT_MODEL]\n",
            "       [--deepspeed_hostfile DEEPSPEED_HOSTFILE]\n",
            "       [--deepspeed_exclusion_filter DEEPSPEED_EXCLUSION_FILTER]\n",
            "       [--deepspeed_inclusion_filter DEEPSPEED_INCLUSION_FILTER]\n",
            "       [--deepspeed_multinode_launcher DEEPSPEED_MULTINODE_LAUNCHER]\n",
            "       [--fsdp_offload_params FSDP_OFFLOAD_PARAMS]\n",
            "       [--fsdp_min_num_params FSDP_MIN_NUM_PARAMS]\n",
            "       [--fsdp_sharding_strategy FSDP_SHARDING_STRATEGY]\n",
            "       [--fsdp_auto_wrap_policy FSDP_AUTO_WRAP_POLICY]\n",
            "       [--fsdp_transformer_layer_cls_to_wrap FSDP_TRANSFORMER_LAYER_CLS_TO_WRAP]\n",
            "       [--fsdp_backward_prefetch_policy FSDP_BACKWARD_PREFETCH_POLICY]\n",
            "       [--fsdp_state_dict_type FSDP_STATE_DICT_TYPE]\n",
            "       [--megatron_lm_tp_degree MEGATRON_LM_TP_DEGREE]\n",
            "       [--megatron_lm_pp_degree MEGATRON_LM_PP_DEGREE]\n",
            "       [--megatron_lm_num_micro_batches MEGATRON_LM_NUM_MICRO_BATCHES]\n",
            "       [--megatron_lm_sequence_parallelism MEGATRON_LM_SEQUENCE_PARALLELISM]\n",
            "       [--megatron_lm_recompute_activations MEGATRON_LM_RECOMPUTE_ACTIVATIONS]\n",
            "       [--megatron_lm_use_distributed_optimizer MEGATRON_LM_USE_DISTRIBUTED_OPTIMIZER]\n",
            "       [--megatron_lm_gradient_clipping MEGATRON_LM_GRADIENT_CLIPPING]\n",
            "       [--aws_access_key_id AWS_ACCESS_KEY_ID]\n",
            "       [--aws_secret_access_key AWS_SECRET_ACCESS_KEY]\n",
            "       [--debug]\n",
            "       training_script\n",
            "       ...\n",
            "accelerate <command> [<args>] launch: error: the following arguments are required: training_script, training_script_args\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4RMLk0YYkT8l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}